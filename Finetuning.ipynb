{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1569144-0557-4314-87e9-7f2231319d9c",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Project: Open Source Institute-Cognitive System of Machine Intelligent Computing (OpenSI-CoSMIC)\n",
    "# Contributors:\n",
    "#     Muntasir Adnan <adnan.adnan@canberra.edu.au>\n",
    "# \n",
    "# Copyright (c) 2025 Open Source Institute\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the \"Software\"), to deal in the Software without restriction, including without\n",
    "# limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so, subject to the following\n",
    "# conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial\n",
    "# portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\n",
    "# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    "# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "# -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba47ca69-c6ae-4c90-9be6-30be1c356780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title(section_num: int, description: str) ->  None:\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Section {section_num}: {description}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1a9b0-cac0-48de-81ed-0a018cdbc933",
   "metadata": {},
   "source": [
    "# SECTION 1: VERIFY ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585e6a8e-1f9c-4a81-88ce-8d20fbf8bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 1: Environment\n",
      "--------------------------------------------------------------------------------\n",
      "Python version: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]\n",
      "Python executable: /home/adnana/miniconda3/envs/llm/bin/python3.10\n",
      "\n",
      "PyTorch version: 2.1.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: NVIDIA A100 80GB PCIe\n",
      "GPU memory: 85.09 GB\n",
      "Current GPU memory allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print_title(1, \"Environment\")\n",
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Current GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\nWARNING: CUDA not available! This tutorial requires a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404c814-754f-4caa-a42a-abec4f36a0d8",
   "metadata": {},
   "source": [
    "# SECTION 2: IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7671ae4d-145c-41c2-b555-e1f9e1dbe31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 2: Importing Libraries\n",
      "--------------------------------------------------------------------------------\n",
      "All imports successful!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_title(2, \"Importing Libraries\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from huggingface_hub import login\n",
    "from jinja2 import Template\n",
    "\n",
    "print(\"All imports successful!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b2b6be-68da-4c81-bc74-624cf58bfcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf login\n",
    "load_dotenv(\".env\")\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf8cbe-d882-463b-9baa-5570bfb642a0",
   "metadata": {},
   "source": [
    "# SECTION 3: LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faeee207-d64f-4bea-9b90-0090c2d3aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 3: Dataset\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset loaded: 100 examples\n",
      "Columns: ['q', 'a']\n",
      "\n",
      "First 3 examples:\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: Write a function to add two numbers\n",
      "Answer: def add_numbers(a, b):\n",
      "    return a + b\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: Create a function to check if a number is even\n",
      "Answer: def is_even(n):\n",
      "    return n % 2 == 0\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Write a function to reverse a string\n",
      "Answer: def reverse_string(s):\n",
      "    return s[::-1]\n",
      "\n",
      "Dataset loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_title(3, \"Dataset\")\n",
    "\n",
    "synthetic_data = {\n",
    "    'q': [\n",
    "        'Write a function to add two numbers',\n",
    "        'Create a function to check if a number is even',\n",
    "        'Write a function to reverse a string',\n",
    "        'Create a function to find the maximum in a list',\n",
    "        'Write a function to calculate factorial',\n",
    "        'Create a function to check if a number is prime',\n",
    "        'Write a function to find the sum of a list',\n",
    "        'Create a function to sort a dictionary by values',\n",
    "        'Write a function to remove duplicates from a list',\n",
    "        'Create a function to check if a string is a palindrome',\n",
    "    ] * 10,\n",
    "    'a': [\n",
    "        'def add_numbers(a, b):\\n    return a + b',\n",
    "        'def is_even(n):\\n    return n % 2 == 0',\n",
    "        'def reverse_string(s):\\n    return s[::-1]',\n",
    "        'def find_max(lst):\\n    return max(lst)',\n",
    "        'def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)',\n",
    "        'def is_prime(n):\\n    if n < 2:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True',\n",
    "        'def sum_list(lst):\\n    return sum(lst)',\n",
    "        'def sort_dict_by_value(d):\\n    return dict(sorted(d.items(), key=lambda x: x[1]))',\n",
    "        'def remove_duplicates(lst):\\n    return list(set(lst))',\n",
    "        'def is_palindrome(s):\\n    return s == s[::-1]',\n",
    "    ] * 10\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} examples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for idx in range(3):\n",
    "    print(f\"\\n--- Example {idx+1} ---\")\n",
    "    print(f\"Question: {df.iloc[idx]['q']}\")\n",
    "    print(f\"Answer: {df.iloc[idx]['a']}\")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eacf56-16aa-4b6d-b7c6-e2c9e92724bf",
   "metadata": {},
   "source": [
    "# SECTION 4: TOKENIZER LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31ad658b-a563-4f92-b677-50d81d3b41ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 4: Tokenizer Loading\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenizer loaded!\n",
      "Vocabulary size: 151,665\n",
      "Model max length: 131072\n",
      "BOS token: 'None' (ID: None)\n",
      "EOS token: '<|endoftext|>' (ID: 151643)\n",
      "PAD token: '<|endoftext|>' (ID: 151643)\n"
     ]
    }
   ],
   "source": [
    "print_title(4, \"Tokenizer Loading\")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "# model_name = \"./qwen\" # offline\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"PAD token is not dest by default for model: {model_name}\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"Tokenizer loaded!\")\n",
    "print(f\"Vocabulary size: {len(tokenizer):,}\")\n",
    "# https://huggingface.co/transformers/v3.2.0/main_classes/tokenizer.html#:~:text=model_max_length%20(%20int%20%2C%20optional%20)%20%E2%80%93,inputs%20to%20the%20transformer%20model.\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"BOS token: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c847cb-8ec7-4620-8904-fe95fa7eaa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca4473-9d65-4ea0-a7f5-9ee2155ac467",
   "metadata": {},
   "source": [
    "# SECTION 5: TOKENIZATION HANDS-ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce5b5217-622c-4291-9591-d0d6d06f91f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 5: Understanding Tokenization\n",
      "--------------------------------------------------------------------------------\n",
      "Question: Write a function to calculate the sum of a list\n",
      "Answer: def sum_list(lst):\n",
      "    return sum(lst)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "QUESTION TOKENIZATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Tokens: ['Write', 'Ġa', 'Ġfunction', 'Ġto', 'Ġcalculate', 'Ġthe', 'Ġsum', 'Ġof', 'Ġa', 'Ġlist']\n",
      "Token IDs: [7985, 264, 729, 311, 11047, 279, 2629, 315, 264, 1140]\n",
      "Number of tokens: 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANSWER TOKENIZATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Tokens: ['def', 'Ġsum', '_list', '(lst', '):Ċ', 'ĠĠĠ', 'Ġreturn', 'Ġsum', '(lst', ')']\n",
      "Token IDs: [750, 2629, 2019, 46046, 982, 262, 470, 2629, 46046, 8]\n",
      "Number of tokens: 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FULL SEQUENCE WITH SPECIAL TOKENS:\n",
      "--------------------------------------------------------------------------------\n",
      "Input IDs shape: torch.Size([1, 25])\n",
      "Input IDs: [14582, 25, 9645, 264, 729, 311, 11047, 279, 2629, 315, 264, 1140, 198, 16141, 25, 707, 2629, 2019, 46046, 982, 262, 470, 2629, 46046, 8]\n",
      "\n",
      "Decoded text: Question: Write a function to calculate the sum of a list\n",
      "Answer: def sum_list(lst):\n",
      "    return sum(lst)\n"
     ]
    }
   ],
   "source": [
    "print_title(5, \"Understanding Tokenization\")\n",
    "\n",
    "example_question = \"Write a function to calculate the sum of a list\"\n",
    "example_answer = \"def sum_list(lst):\\n    return sum(lst)\"\n",
    "\n",
    "print(f\"Question: {example_question}\")\n",
    "print(f\"Answer: {example_answer}\")\n",
    "\n",
    "q_tokens = tokenizer.tokenize(example_question)\n",
    "q_ids = tokenizer.encode(example_question, add_special_tokens=False)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"QUESTION TOKENIZATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Tokens: {q_tokens}\")\n",
    "print(f\"Token IDs: {q_ids}\")\n",
    "print(f\"Number of tokens: {len(q_ids)}\")\n",
    "\n",
    "a_tokens = tokenizer.tokenize(example_answer)\n",
    "a_ids = tokenizer.encode(example_answer, add_special_tokens=False)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ANSWER TOKENIZATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Tokens: {a_tokens}\")\n",
    "print(f\"Token IDs: {a_ids}\")\n",
    "print(f\"Number of tokens: {len(a_ids)}\")\n",
    "\n",
    "# bytre pair encoding: BPE builds its vocabulary by merging frequently co-occurring byte sequences.\n",
    "full_text = f\"Question: {example_question}\\nAnswer: {example_answer}\"\n",
    "full_encoding = tokenizer(full_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FULL SEQUENCE WITH SPECIAL TOKENS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Input IDs shape: {full_encoding['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {full_encoding['input_ids'][0].tolist()}\")\n",
    "\n",
    "decoded = tokenizer.decode(full_encoding['input_ids'][0])\n",
    "print(f\"\\nDecoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f04f6-0a69-49e8-98d5-c324a11eb100",
   "metadata": {},
   "source": [
    "# SECTION 6: QUANTIZATION CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7245db2b-56ef-47d4-ba89-90ad29eecab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 6: Quantization Configuration\n",
      "--------------------------------------------------------------------------------\n",
      "Configuring 4-bit quantization...\n"
     ]
    }
   ],
   "source": [
    "print_title(6, \"Quantization Configuration\")\n",
    "\n",
    "print(\"Configuring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",              # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for stability\n",
    "    bnb_4bit_use_double_quant=True,         # Double quantization for more memory savings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6ef42-1fec-413b-8c22-76b1bdbfd7ee",
   "metadata": {},
   "source": [
    "# SECTION 7: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e8e80b-24e1-40af-9980-d8d02f25774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/Qwen/Qwen2.5-0.5B/resolve/main/model.safetensors?download=true ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7676d18-aa66-4e70-a455-5c38853362f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 7: Model Loading\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# does not work with ipykernel 7.0.1 \n",
    "# downgrade to 6.30.1\n",
    "# https://discuss.huggingface.co/t/model-loading-gets-stuck-when-calling-from-pretrained/112807/11\n",
    "\n",
    "print_title(7, \"Model Loading\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=\"./qwen_cache\"\n",
    ")\n",
    "\n",
    "# Shards would divide the tensor files into multiple files.\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     local_files_only=True  # Add this to prevent any download attempts\n",
    "# )\n",
    "\n",
    "# print(\"\\nModel loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669af4c5-7a02-4aca-8925-83aa4ba79435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL ARCHITECTURE OVERVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MODEL ARCHITECTURE OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0ddc2-4c9a-40a0-8fd7-f6cd116d8df0",
   "metadata": {},
   "source": [
    "## Understanding the Model Architetcure  \n",
    "1. TOP LEVEL STRUCTURE:  \n",
    "   • Qwen2ForCausalLM - The complete model for causal language modelling. Predicts the next token autoregressively (one at a time) given previous tokens: P(x_n | x_<n)  \n",
    "   • model - The main transformer model (decoder only)  \n",
    "   • lm_head - Final linear layer that produces vocabulary logits, raw probability score from the hidden layers.  \n",
    "  \n",
    "2. EMBEDDING LAYER:  \n",
    "   • embed_tokens: Embedding(151936, 896)  \n",
    "   • 151936 = vocabulary size (number of unique tokens)  \n",
    "   • 896 = embedding dimension (d_model)  \n",
    "   • Converts token IDs → dense vectors  \n",
    "  \n",
    "3. TRANSFORMER LAYERS:  \n",
    "   • layers: ModuleList (0-23) = 24 decoder layers  \n",
    "   • Each layer has the SAME structure, stacked 24 times  \n",
    "  \n",
    "4. FOR DECODER_LAYER in \"TRANSFORMER LAYERS\":  \n",
    "  \n",
    "   A. SELF-ATTENTION (Qwen2Attention):  \n",
    "      • q_proj: Query projection [896 → 896] \"what am I looking for?\" (learnt)  \n",
    "      • k_proj: Key projection [896 → 128] \"what do I contain?\" (learnt)  \n",
    "      • v_proj: Value projection [896 → 128] \"what information do I carry?\" (learnt)  \n",
    "      • o_proj: Output projection [896 → 896] Combines attention outputs back to model dimension (concat).\n",
    "   k_proj and v_proj are smaller (128) - this is called Multi-Query Attention (MQA) or Grouped-Query Attention (GQA). (shared k and v)  \n",
    "      • rotary_emb: Rotary Position Embedding (RoPE): Encodes position information directly in the attention mechanism  \n",
    "\n",
    "   B. FEED-FORWARD NETWORK (Qwen2MLP):  \n",
    "      • gate_proj: [896 → 4864] - \"Gating\" pathway  \n",
    "      • up_proj: [896 → 4864] - \"Up\" projection  \n",
    "      • down_proj: [4864 → 896] - \"Down\" projection back to d_model  \n",
    "      • act_fn: SiLU() - Activation function (Sigmoid Linear Unit)  \n",
    "\n",
    "   C. LAYER NORMALIZATION:  \n",
    "      • input_layernorm: Qwen2RMSNorm: Applied BEFORE self-attention (Pre-LN)  \n",
    "      • post_attention_layernorm: Qwen2RMSNorm: Applied BEFORE FFN  \n",
    "  \n",
    "6. FINAL LAYERS:\n",
    "   • norm: Qwen2RMSNorm - Final normalization before output  \n",
    "   • lm_head: Linear [896 → 151936]  \n",
    "     - Projects back to vocabulary size  \n",
    "     - Each position gets a score for every possible token (logit)  \n",
    "\n",
    "7. THE \"Linear4bit\":  \n",
    "   • These are quantized layers (from bitsandbytes)  \n",
    "   • Store weights in 4-bit instead of 16-bit  \n",
    "   • Normal: Linear(...)  \n",
    "   • Quantized: Linear4bit(...)  \n",
    "  \n",
    "8. INFORMATION FLOW:  \n",
    "   Token IDs > Embeddings > Decoder Layer 1 > Layer 2 > ... > Layer 24 > Norm > LM Head > Logits  \n",
    "   \n",
    "9. WHERE LoRA WILL BE APPLIED:  \n",
    "   When we add LoRA adapters, we'll target:  \n",
    "   • q_proj, k_proj, v_proj (attention projections)  \n",
    "   • o_proj (attention output)  \n",
    "   • gate_proj, up_proj, down_proj (FFN layers)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3246c2f4-5cde-4bb5-96ca-17e26ab8b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 315,119,488 (0.32B)\n",
      "Model memory footprint: 0.45 GB\n",
      "Available GPU memory: 85.0948 GB\n",
      "Memory usage: 0.5% of 85.0948GB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "\n",
    "# Memory footprint\n",
    "memory_footprint = model.get_memory_footprint() / 1e9\n",
    "print(f\"Model memory footprint: {memory_footprint:.2f} GB\")\n",
    "available_gpu = round(torch.cuda.get_device_properties(0).total_memory / 1e9, 4)\n",
    "print(f\"Available GPU memory: {available_gpu} GB\")\n",
    "print(f\"Memory usage: {memory_footprint / available_gpu * 100:.1f}% of {available_gpu}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886daeb-00da-40a5-8f5f-311833b560a4",
   "metadata": {},
   "source": [
    "# SECTION 7: DATASET FORMATTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44c13d1-a616-48cd-b75b-2127eb1b3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 7: Dataset Formatting\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenizer has built-in chat template!\n"
     ]
    }
   ],
   "source": [
    "print_title(7, \"Dataset Formatting\")\n",
    "\n",
    "# chat template\n",
    "def load_chat_template() -> None:\n",
    "    if tokenizer.chat_template is None:\n",
    "        print(\"Chat template does not exist, loading default chat template.\")\n",
    "        with open(\"chat_template.jinja2\", \"r\") as f:\n",
    "            template_string = f.read()\n",
    "        tokenizer.chat_template = template_string\n",
    "    else:\n",
    "        print(\"Tokenizer has built-in chat template!\")\n",
    "\n",
    "\n",
    "load_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "595d09f8-2796-413d-9938-acc1b9d2c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Template preview:\n",
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTemplate preview:\")\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e39c4df3-d704-4fa2-843e-44aaba4479c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic, industry standard, portable, flexible\n",
    "def format_with_chat_template(question: str, answer: str, tokenize: bool = False, add_generation_prompt: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Example formatting function to work with pandas.\n",
    "    See masking for actual formatting func that we will use for training\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Python coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    \n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = tokenize, # Return string, not token IDs\n",
    "        add_generation_prompt = add_generation_prompt # Don't add generation prompt\n",
    "    )\n",
    "    \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f41c4aec-4094-4991-a5a4-f5dddb33f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful Python coding assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a function to add two numbers<|im_end|>\n",
      "<|im_start|>assistant\n",
      "def add_numbers(a, b):\n",
      "    return a + b<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# formatted example\n",
    "print(format_with_chat_template(df.iloc[0][\"q\"], df.iloc[0][\"a\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7c23a4b-823c-4187-b87b-00fe9df667d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful Python coding assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a function to add two numbers<|im_end|>\n",
      "<|im_start|>assistant\n",
      "def add_numbers(a, b):\n",
      "    return a + b<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# formatted example with gen prompt\n",
    "# for inference\n",
    "print(format_with_chat_template(df.iloc[0][\"q\"], df.iloc[0][\"a\"], add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6576d2e0-a6e0-42a4-aabb-0c7876bdb069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# BOS token: 'None' (ID: None)\n",
    "# EOS token: '<|endoftext|>' (ID: 151643)\n",
    "# PAD token: '<|endoftext|>' (ID: 151643)\n",
    "# ========================================\n",
    "\n",
    "# weird part is qwen's default chat template does not use its set EOS or PAD token\n",
    "print(tokenizer.decode(151644), tokenizer.decode(151645))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e8214a6-ecda-488d-8803-5a5ff2167143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset ready: 100 formatted examples\n"
     ]
    }
   ],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(f\"\\nDataset ready: {len(dataset)} formatted examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e40009-92b0-4e0a-8e1e-c73e3570e400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': ['Write a function to add two numbers',\n",
       "  'Create a function to check if a number is even',\n",
       "  'Write a function to reverse a string',\n",
       "  'Create a function to find the maximum in a list',\n",
       "  'Write a function to calculate factorial'],\n",
       " 'a': ['def add_numbers(a, b):\\n    return a + b',\n",
       "  'def is_even(n):\\n    return n % 2 == 0',\n",
       "  'def reverse_string(s):\\n    return s[::-1]',\n",
       "  'def find_max(lst):\\n    return max(lst)',\n",
       "  'def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538caeb8-3482-4b83-91f4-8daa65a89cef",
   "metadata": {},
   "source": [
    "# SECTION 8: MASKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c037cd88-1a5b-4f98-8a52-4aded28e99d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 8: Masking\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- FORMATTED EXAMPLE (String) ---\n",
      "<|im_start|>system\n",
      "You are a helpful Python coding assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a function to add two numbers<|im_end|>\n",
      "<|im_start|>assistant\n",
      "def add_numbers(a, b):\n",
      "    return a + b<|im_end|>\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_title(8, \"Masking\")\n",
    "\n",
    "# SFT trainer takes care of this automatically, using roles from the chat template but we will demonstrate the process here.\n",
    "# https://huggingface.co/docs/trl/sft_trainer\n",
    "\n",
    "def formatting_func(example: dict) -> str:\n",
    "    \"\"\"\n",
    "    Example formatting function to work with pandas.\n",
    "    See masking for actual formatting func that we will use for training\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Python coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": example[\"q\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"a\"]}\n",
    "    ]\n",
    "    \n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = False\n",
    "    )\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "example_text = formatting_func(dataset[0])\n",
    "print(\"\\n--- FORMATTED EXAMPLE (String) ---\")\n",
    "print(example_text)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108baca0-add4-4773-b787-6296cfe11496",
   "metadata": {},
   "source": [
    "## This is used for calculating loss, when labelled as -100, SFTTrainer will execmpt those tokens while calculating loss. This is basically called training on completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b703f344-76d2-4e3f-aa23-20bfd9fa46e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without target signpost\n",
      "--------------------------------------------------\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "test<|im_end|>\n",
      "\n",
      "With target signpost\n",
      "--------------------------------------------------\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "test<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Signpost text: <|im_start|>assistant\n",
      "\n",
      "Signpost id: {'input_ids': tensor([[151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "Decoded signpost: <|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Signpost is the signal for sft trainer to know where assistant response starts.\n",
    "# So sample with asssitant response - sameple without would be the signpost\n",
    "# We find the \"signpost\" by seeing what tokens are added by add_generation_prompt=True\n",
    "\n",
    "dummy_history = [{\"role\": \"user\", \"content\": \"test\"}]\n",
    "\n",
    "templated_without_prompt = tokenizer.apply_chat_template(\n",
    "    dummy_history,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "print(\"Without target signpost\")\n",
    "print(\"-\"*50)\n",
    "print(templated_without_prompt)\n",
    "templated_with_prompt = tokenizer.apply_chat_template(\n",
    "    dummy_history,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"With target signpost\")\n",
    "print(\"-\"*50)\n",
    "print(templated_with_prompt)\n",
    "\n",
    "signpost_text = templated_with_prompt[len(templated_without_prompt):]\n",
    "print(f\"Signpost text: {signpost_text}\")\n",
    "signpost_id = tokenizer(signpost_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "print(f\"Signpost id: {signpost_id}\")\n",
    "\n",
    "decoded_signpost = tokenizer.decode(signpost_id[\"input_ids\"][0].tolist()) # to check\n",
    "print(f\"Decoded signpost: {decoded_signpost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48a6419a-5a25-4b55-bd8f-5206a87e5fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying the DataCollator...\n",
      "\n",
      "--- MASKING DEMONSTRATION ---\n",
      "           Token  Input ID   Label  Attention Mask\n",
      "0   <|im_start|>    151644    -100               1\n",
      "1         system      8948    -100               1\n",
      "2              Ċ       198    -100               1\n",
      "3            You      2610    -100               1\n",
      "4           Ġare       525    -100               1\n",
      "5             Ġa       264    -100               1\n",
      "6       Ġhelpful     10950    -100               1\n",
      "7        ĠPython     13027    -100               1\n",
      "8        Ġcoding     10822    -100               1\n",
      "9     Ġassistant     17847    -100               1\n",
      "10             .        13    -100               1\n",
      "11    <|im_end|>    151645    -100               1\n",
      "12             Ċ       198    -100               1\n",
      "13  <|im_start|>    151644    -100               1\n",
      "14          user       872    -100               1\n",
      "15             Ċ       198    -100               1\n",
      "16         Write      7985    -100               1\n",
      "17            Ġa       264    -100               1\n",
      "18     Ġfunction       729    -100               1\n",
      "19           Ġto       311    -100               1\n",
      "20          Ġadd       912    -100               1\n",
      "21          Ġtwo      1378    -100               1\n",
      "22      Ġnumbers      5109    -100               1\n",
      "23    <|im_end|>    151645    -100               1\n",
      "24             Ċ       198    -100               1\n",
      "25  <|im_start|>    151644    -100               1\n",
      "26     assistant     77091    -100               1\n",
      "27             Ċ       198    -100               1\n",
      "28           def       750     750               1\n",
      "29          Ġadd       912     912               1\n",
      "30      _numbers     32964   32964               1\n",
      "31            (a      2877    2877               1\n",
      "32             ,        11      11               1\n",
      "33            Ġb       293     293               1\n",
      "34           ):Ċ       982     982               1\n",
      "35           ĠĠĠ       262     262               1\n",
      "36       Ġreturn       470     470               1\n",
      "37            Ġa       264     264               1\n",
      "38            Ġ+       488     488               1\n",
      "39            Ġb       293     293               1\n",
      "40    <|im_end|>    151645  151645               1\n",
      "41             Ċ       198     198               1\n"
     ]
    }
   ],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=signpost_id[\"input_ids\"][0].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "tokenized_example = tokenizer(example_text, return_tensors=\"pt\")\n",
    "\n",
    "batch = [\n",
    "    {\n",
    "        \"input_ids\": tokenized_example[\"input_ids\"][0],\n",
    "        \"attention_mask\": tokenized_example[\"attention_mask\"][0]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Applying the DataCollator...\")\n",
    "processed_batch = collator(batch)\n",
    "\n",
    "# print(processed_batch)\n",
    "\n",
    "print(\"\\n--- MASKING DEMONSTRATION ---\")\n",
    "\n",
    "df_comparison = pd.DataFrame({\n",
    "    \"Token\": tokenizer.convert_ids_to_tokens(processed_batch[\"input_ids\"][0]),\n",
    "    \"Input ID\": processed_batch[\"input_ids\"][0].tolist(),\n",
    "    \"Label\": processed_batch[\"labels\"][0].tolist(),\n",
    "    \"Attention Mask\": processed_batch[\"attention_mask\"][0].tolist()\n",
    "})\n",
    "\n",
    "print(df_comparison.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587ba41-d7a3-49f2-996e-65902d920088",
   "metadata": {},
   "source": [
    "# SECTION 9: ATTENTION MASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdf276-9108-485b-a713-5896c867ee2f",
   "metadata": {},
   "source": [
    "## The 'attention_mask' tells the model what to LOOK at. This is used to handle 'padding', which is added when we batch sequences of different lengths so they all have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f305142-489c-4530-86dc-b6182656aeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 9: Attention Mask\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Example 1 ---\n",
      "Write a function to add two numbers\n",
      "Length: 7 tokens\n",
      "\n",
      "--- Example 2 ---\n",
      "Write a function to check if a string is a palindrome. It should be efficient.\n",
      "Length: 17 tokens\n"
     ]
    }
   ],
   "source": [
    "print_title(9, \"Attention Mask\")\n",
    "\n",
    "text1 = \"Write a function to add two numbers\"\n",
    "text2 = \"Write a function to check if a string is a palindrome. It should be efficient.\"\n",
    "\n",
    "print(f\"\\n--- Example 1 ---\\n{text1}\")\n",
    "print(f\"Length: {len(tokenizer.encode(text1))} tokens\")\n",
    "\n",
    "print(f\"\\n--- Example 2 ---\\n{text2}\")\n",
    "print(f\"Length: {len(tokenizer.encode(text2))} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6314e115-e9b0-4fe4-a2a0-5ee3e906e6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  7985,    264,    729,    311,    912,   1378,   5109, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
       "        [  7985,    264,    729,    311,   1779,    421,    264,    914,    374,\n",
       "            264,  73097,     13,   1084,   1265,    387,  11050,     13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs = tokenizer(\n",
    "    [text1, text2], \n",
    "    padding=\"longest\", # padding will be added because of this\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "# PAD token: '<|endoftext|>' (ID: 151643)\n",
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b9602d1-6c6b-4c59-8302-e1355916148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD Token ID: 151643\n",
      "\n",
      "Decoded Input IDs:\n",
      "Example 1: Write a function to add two numbers<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Example 2: Write a function to check if a string is a palindrome. It should be efficient.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Attention Mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"PAD Token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "print(\"\\nDecoded Input IDs:\")\n",
    "print(\"Example 1:\", tokenizer.decode(batch_inputs[\"input_ids\"][0]))\n",
    "print(\"Example 2:\", tokenizer.decode(batch_inputs[\"input_ids\"][1]))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Show the attention_mask\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(batch_inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ac985-209c-4d10-acda-82c9da58a99a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
