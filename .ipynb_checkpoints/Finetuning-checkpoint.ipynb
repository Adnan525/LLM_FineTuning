{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb631cd5-951a-4fad-9820-29577c34e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Project: Open Source Institute-Cognitive System of Machine Intelligent Computing (OpenSI-CoSMIC)\n",
    "# Contributors:\n",
    "#     Muntasir Adnan <adnan.adnan@canberra.edu.au>\n",
    "# \n",
    "# Copyright (c) 2025 Open Source Institute\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the \"Software\"), to deal in the Software without restriction, including without\n",
    "# limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so, subject to the following\n",
    "# conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial\n",
    "# portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\n",
    "# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    "# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "# -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba47ca69-c6ae-4c90-9be6-30be1c356780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title(section_num: int, description: str) ->  None:\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Section {section_num}: {description}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1a9b0-cac0-48de-81ed-0a018cdbc933",
   "metadata": {},
   "source": [
    "# SECTION 1: VERIFY ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585e6a8e-1f9c-4a81-88ce-8d20fbf8bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 1: Environment\n",
      "--------------------------------------------------------------------------------\n",
      "Python version: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]\n",
      "Python executable: /home/adnana/miniconda3/envs/llm/bin/python3.10\n",
      "\n",
      "PyTorch version: 2.1.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: NVIDIA A100 80GB PCIe\n",
      "GPU memory: 85.09 GB\n",
      "Current GPU memory allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print_title(1, \"Environment\")\n",
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Current GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\nWARNING: CUDA not available! This tutorial requires a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404c814-754f-4caa-a42a-abec4f36a0d8",
   "metadata": {},
   "source": [
    "# SECTION 2: IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7671ae4d-145c-41c2-b555-e1f9e1dbe31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 2: Importing Libraries\n",
      "--------------------------------------------------------------------------------\n",
      "All imports successful!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_title(2, \"Importing Libraries\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All imports successful!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf8cbe-d882-463b-9baa-5570bfb642a0",
   "metadata": {},
   "source": [
    "# SECTION 3: LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faeee207-d64f-4bea-9b90-0090c2d3aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 3: Dataset\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset loaded: 100 examples\n",
      "Columns: ['q', 'a']\n",
      "\n",
      "First 3 examples:\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: Write a function to add two numbers\n",
      "Answer: def add_numbers(a, b):\n",
      "    return a + b\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: Create a function to check if a number is even\n",
      "Answer: def is_even(n):\n",
      "    return n % 2 == 0\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Write a function to reverse a string\n",
      "Answer: def reverse_string(s):\n",
      "    return s[::-1]\n",
      "\n",
      "Dataset loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_title(3, \"Dataset\")\n",
    "\n",
    "synthetic_data = {\n",
    "    'q': [\n",
    "        'Write a function to add two numbers',\n",
    "        'Create a function to check if a number is even',\n",
    "        'Write a function to reverse a string',\n",
    "        'Create a function to find the maximum in a list',\n",
    "        'Write a function to calculate factorial',\n",
    "        'Create a function to check if a number is prime',\n",
    "        'Write a function to find the sum of a list',\n",
    "        'Create a function to sort a dictionary by values',\n",
    "        'Write a function to remove duplicates from a list',\n",
    "        'Create a function to check if a string is a palindrome',\n",
    "    ] * 10,\n",
    "    'a': [\n",
    "        'def add_numbers(a, b):\\n    return a + b',\n",
    "        'def is_even(n):\\n    return n % 2 == 0',\n",
    "        'def reverse_string(s):\\n    return s[::-1]',\n",
    "        'def find_max(lst):\\n    return max(lst)',\n",
    "        'def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)',\n",
    "        'def is_prime(n):\\n    if n < 2:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True',\n",
    "        'def sum_list(lst):\\n    return sum(lst)',\n",
    "        'def sort_dict_by_value(d):\\n    return dict(sorted(d.items(), key=lambda x: x[1]))',\n",
    "        'def remove_duplicates(lst):\\n    return list(set(lst))',\n",
    "        'def is_palindrome(s):\\n    return s == s[::-1]',\n",
    "    ] * 10\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} examples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for idx in range(3):\n",
    "    print(f\"\\n--- Example {idx+1} ---\")\n",
    "    print(f\"Question: {df.iloc[idx]['q']}\")\n",
    "    print(f\"Answer: {df.iloc[idx]['a']}\")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eacf56-16aa-4b6d-b7c6-e2c9e92724bf",
   "metadata": {},
   "source": [
    "# SECTION 4: TOKENIZER LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ad658b-a563-4f92-b677-50d81d3b41ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 4: Tokenizer Loading\n",
      "--------------------------------------------------------------------------------\n",
      "PAD token is not dest by default for model: microsoft/phi-2\n",
      "Tokenizer loaded!\n",
      "Vocabulary size: 50,295\n",
      "Model max length: 2048\n",
      "BOS token: '<|endoftext|>' (ID: 50256)\n",
      "EOS token: '<|endoftext|>' (ID: 50256)\n",
      "PAD token: '<|endoftext|>' (ID: 50256)\n"
     ]
    }
   ],
   "source": [
    "print_title(4, \"Tokenizer Loading\")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"./model_cache\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"PAD token is not dest by default for model: {model_name}\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"Tokenizer loaded!\")\n",
    "print(f\"Vocabulary size: {len(tokenizer):,}\")\n",
    "# https://huggingface.co/transformers/v3.2.0/main_classes/tokenizer.html#:~:text=model_max_length%20(%20int%20%2C%20optional%20)%20%E2%80%93,inputs%20to%20the%20transformer%20model.\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"BOS token: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c847cb-8ec7-4620-8904-fe95fa7eaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f04f6-0a69-49e8-98d5-c324a11eb100",
   "metadata": {},
   "source": [
    "# SECTION 5: QUANTIZATION CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7245db2b-56ef-47d4-ba89-90ad29eecab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 5: Quantization Configuration\n",
      "--------------------------------------------------------------------------------\n",
      "Configuring 4-bit quantization...\n"
     ]
    }
   ],
   "source": [
    "print_title(5, \"Quantization Configuration\")\n",
    "\n",
    "print(\"Configuring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",              # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for stability\n",
    "    bnb_4bit_use_double_quant=True,         # Double quantization for more memory savings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6ef42-1fec-413b-8c22-76b1bdbfd7ee",
   "metadata": {},
   "source": [
    "# SECTION 5: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7676d18-aa66-4e70-a455-5c38853362f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbdf3f739fa4c3e80b384b651de0519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb348eba6394f038aef4da4fe40b301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",                      # Automatically place on GPU\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=\"./model_cache\"\n",
    ")\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bf730-7d23-4634-b8ab-660e7be6ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MODEL ARCHITECTURE OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246c2f4-5cde-4bb5-96ca-17e26ab8b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "\n",
    "# Memory footprint\n",
    "memory_footprint = model.get_memory_footprint() / 1e9\n",
    "print(f\"Model memory footprint: {memory_footprint:.2f} GB\")\n",
    "available_gpu = torch.cuda.get_device_properties(0).total_memory / 1e9:.2f\n",
    "print(f\"Available GPU memory: {available_gpu} GB\")\n",
    "print(f\"Memory usage: {memory_footprint / available_gpu * 100:.1f}% of {available_gpu}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
