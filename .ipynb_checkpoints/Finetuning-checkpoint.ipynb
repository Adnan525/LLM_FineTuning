{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb631cd5-951a-4fad-9820-29577c34e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Project: Open Source Institute-Cognitive System of Machine Intelligent Computing (OpenSI-CoSMIC)\n",
    "# Contributors:\n",
    "#     Muntasir Adnan <adnan.adnan@canberra.edu.au>\n",
    "# \n",
    "# Copyright (c) 2025 Open Source Institute\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the \"Software\"), to deal in the Software without restriction, including without\n",
    "# limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so, subject to the following\n",
    "# conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial\n",
    "# portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\n",
    "# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    "# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "# -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba47ca69-c6ae-4c90-9be6-30be1c356780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title(section_num: int, description: str) ->  None:\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Section {section_num}: {description}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1a9b0-cac0-48de-81ed-0a018cdbc933",
   "metadata": {},
   "source": [
    "# SECTION 1: VERIFY ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585e6a8e-1f9c-4a81-88ce-8d20fbf8bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 1: Environment\n",
      "--------------------------------------------------------------------------------\n",
      "Python version: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]\n",
      "Python executable: /home/adnana/miniconda3/envs/llm/bin/python3.10\n",
      "\n",
      "PyTorch version: 2.1.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: NVIDIA A100 80GB PCIe\n",
      "GPU memory: 85.09 GB\n",
      "Current GPU memory allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print_title(1, \"Environment\")\n",
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Current GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\nWARNING: CUDA not available! This tutorial requires a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404c814-754f-4caa-a42a-abec4f36a0d8",
   "metadata": {},
   "source": [
    "# SECTION 2: IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7671ae4d-145c-41c2-b555-e1f9e1dbe31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 2: Importing Libraries\n",
      "--------------------------------------------------------------------------------\n",
      "All imports successful!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_title(2, \"Importing Libraries\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All imports successful!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf8cbe-d882-463b-9baa-5570bfb642a0",
   "metadata": {},
   "source": [
    "# SECTION 3: LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faeee207-d64f-4bea-9b90-0090c2d3aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 3: Dataset\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset loaded: 100 examples\n",
      "Columns: ['q', 'a']\n",
      "\n",
      "First 3 examples:\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: Write a function to add two numbers\n",
      "Answer: def add_numbers(a, b):\n",
      "    return a + b\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: Create a function to check if a number is even\n",
      "Answer: def is_even(n):\n",
      "    return n % 2 == 0\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Write a function to reverse a string\n",
      "Answer: def reverse_string(s):\n",
      "    return s[::-1]\n",
      "\n",
      "Dataset loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_title(3, \"Dataset\")\n",
    "\n",
    "synthetic_data = {\n",
    "    'q': [\n",
    "        'Write a function to add two numbers',\n",
    "        'Create a function to check if a number is even',\n",
    "        'Write a function to reverse a string',\n",
    "        'Create a function to find the maximum in a list',\n",
    "        'Write a function to calculate factorial',\n",
    "        'Create a function to check if a number is prime',\n",
    "        'Write a function to find the sum of a list',\n",
    "        'Create a function to sort a dictionary by values',\n",
    "        'Write a function to remove duplicates from a list',\n",
    "        'Create a function to check if a string is a palindrome',\n",
    "    ] * 10,\n",
    "    'a': [\n",
    "        'def add_numbers(a, b):\\n    return a + b',\n",
    "        'def is_even(n):\\n    return n % 2 == 0',\n",
    "        'def reverse_string(s):\\n    return s[::-1]',\n",
    "        'def find_max(lst):\\n    return max(lst)',\n",
    "        'def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)',\n",
    "        'def is_prime(n):\\n    if n < 2:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True',\n",
    "        'def sum_list(lst):\\n    return sum(lst)',\n",
    "        'def sort_dict_by_value(d):\\n    return dict(sorted(d.items(), key=lambda x: x[1]))',\n",
    "        'def remove_duplicates(lst):\\n    return list(set(lst))',\n",
    "        'def is_palindrome(s):\\n    return s == s[::-1]',\n",
    "    ] * 10\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} examples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for idx in range(3):\n",
    "    print(f\"\\n--- Example {idx+1} ---\")\n",
    "    print(f\"Question: {df.iloc[idx]['q']}\")\n",
    "    print(f\"Answer: {df.iloc[idx]['a']}\")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eacf56-16aa-4b6d-b7c6-e2c9e92724bf",
   "metadata": {},
   "source": [
    "# SECTION 4: TOKENIZER LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ad658b-a563-4f92-b677-50d81d3b41ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 4: Tokenizer Loading\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenizer loaded!\n",
      "Vocabulary size: 151,665\n",
      "Model max length: 131072\n",
      "BOS token: 'None' (ID: None)\n",
      "EOS token: '<|endoftext|>' (ID: 151643)\n",
      "PAD token: '<|endoftext|>' (ID: 151643)\n"
     ]
    }
   ],
   "source": [
    "print_title(4, \"Tokenizer Loading\")\n",
    "\n",
    "model_name = \"./qwen\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"PAD token is not dest by default for model: {model_name}\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"Tokenizer loaded!\")\n",
    "print(f\"Vocabulary size: {len(tokenizer):,}\")\n",
    "# https://huggingface.co/transformers/v3.2.0/main_classes/tokenizer.html#:~:text=model_max_length%20(%20int%20%2C%20optional%20)%20%E2%80%93,inputs%20to%20the%20transformer%20model.\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"BOS token: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c847cb-8ec7-4620-8904-fe95fa7eaa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='./qwen', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f04f6-0a69-49e8-98d5-c324a11eb100",
   "metadata": {},
   "source": [
    "# SECTION 5: QUANTIZATION CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7245db2b-56ef-47d4-ba89-90ad29eecab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 5: Quantization Configuration\n",
      "--------------------------------------------------------------------------------\n",
      "Configuring 4-bit quantization...\n"
     ]
    }
   ],
   "source": [
    "print_title(5, \"Quantization Configuration\")\n",
    "\n",
    "print(\"Configuring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",              # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for stability\n",
    "    bnb_4bit_use_double_quant=True,         # Double quantization for more memory savings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6ef42-1fec-413b-8c22-76b1bdbfd7ee",
   "metadata": {},
   "source": [
    "# SECTION 6: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e8e80b-24e1-40af-9980-d8d02f25774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/Qwen/Qwen2.5-0.5B/resolve/main/model.safetensors?download=true ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7676d18-aa66-4e70-a455-5c38853362f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     cache_dir=\"./qwen\"\n",
    "# )\n",
    "\n",
    "# Shards would divide the tensor files into multiple files.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True  # Add this to prevent any download attempts\n",
    ")\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "669af4c5-7a02-4aca-8925-83aa4ba79435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL ARCHITECTURE OVERVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MODEL ARCHITECTURE OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0ddc2-4c9a-40a0-8fd7-f6cd116d8df0",
   "metadata": {},
   "source": [
    "1. TOP LEVEL STRUCTURE:  \n",
    "   • Qwen2ForCausalLM - The complete model for causal language modelling. Predicts the next token autoregressively (one at a time) given previous tokens: P(x_n | x_<n)  \n",
    "   • model - The main transformer model (decoder only)  \n",
    "   • lm_head - Final linear layer that produces vocabulary logits, raw probability score from the hidden layers.  \n",
    "  \n",
    "2. EMBEDDING LAYER:  \n",
    "   • embed_tokens: Embedding(151936, 896)  \n",
    "   • 151936 = vocabulary size (number of unique tokens)  \n",
    "   • 896 = embedding dimension (d_model)  \n",
    "   • Converts token IDs → dense vectors  \n",
    "  \n",
    "3. TRANSFORMER LAYERS:  \n",
    "   • layers: ModuleList (0-23) = 24 decoder layers  \n",
    "   • Each layer has the SAME structure, stacked 24 times  \n",
    "  \n",
    "4. FOR DECODER_LAYER in \"TRANSFORMER LAYERS\":  \n",
    "  \n",
    "   A. SELF-ATTENTION (Qwen2Attention):  \n",
    "      • q_proj: Query projection [896 → 896] \"what am I looking for?\" (learnt)  \n",
    "      • k_proj: Key projection [896 → 128] \"what do I contain?\" (learnt)  \n",
    "      • v_proj: Value projection [896 → 128] \"what information do I carry?\" (learnt)  \n",
    "      • o_proj: Output projection [896 → 896] Combines attention outputs back to model dimension (concat).\n",
    "   k_proj and v_proj are smaller (128) - this is called Multi-Query Attention (MQA) or Grouped-Query Attention (GQA). (shared k and v)  \n",
    "      • rotary_emb: Rotary Position Embedding (RoPE): Encodes position information directly in the attention mechanism  \n",
    "\n",
    "   B. FEED-FORWARD NETWORK (Qwen2MLP):\n",
    "      • gate_proj: [896 → 4864] - \"Gating\" pathway  \n",
    "      • up_proj: [896 → 4864] - \"Up\" projection  \n",
    "      • down_proj: [4864 → 896] - \"Down\" projection back to d_model  \n",
    "      • act_fn: SiLU() - Activation function (Sigmoid Linear Unit)  \n",
    "\n",
    "   C. LAYER NORMALIZATION:\n",
    "      • input_layernorm: Qwen2RMSNorm: Applied BEFORE self-attention (Pre-LN)  \n",
    "      • post_attention_layernorm: Qwen2RMSNorm: Applied BEFORE FFN  \n",
    "  \n",
    "6. FINAL LAYERS:\n",
    "   • norm: Qwen2RMSNorm - Final normalization before output  \n",
    "   • lm_head: Linear [896 → 151936]  \n",
    "     - Projects back to vocabulary size  \n",
    "     - Each position gets a score for every possible token (logit)  \n",
    "\n",
    "7. THE \"Linear4bit\":  \n",
    "   • These are quantized layers (from bitsandbytes)  \n",
    "   • Store weights in 4-bit instead of 16-bit  \n",
    "   • Normal: Linear(...)  \n",
    "   • Quantized: Linear4bit(...)  \n",
    "  \n",
    "8. INFORMATION FLOW:  \n",
    "   Token IDs > Embeddings > Decoder Layer 1 > Layer 2 > ... > Layer 24 > Norm > LM Head > Logits  \n",
    "   \n",
    "9. WHERE LoRA WILL BE APPLIED:  \n",
    "   When we add LoRA adapters, we'll target:  \n",
    "   • q_proj, k_proj, v_proj (attention projections)  \n",
    "   • o_proj (attention output)  \n",
    "   • gate_proj, up_proj, down_proj (FFN layers)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3246c2f4-5cde-4bb5-96ca-17e26ab8b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 315,119,488 (0.32B)\n",
      "Model memory footprint: 0.45 GB\n",
      "Available GPU memory: 85.0948 GB\n",
      "Memory usage: 0.5% of 85.0948GB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "\n",
    "# Memory footprint\n",
    "memory_footprint = model.get_memory_footprint() / 1e9\n",
    "print(f\"Model memory footprint: {memory_footprint:.2f} GB\")\n",
    "available_gpu = round(torch.cuda.get_device_properties(0).total_memory / 1e9, 4)\n",
    "print(f\"Available GPU memory: {available_gpu} GB\")\n",
    "print(f\"Memory usage: {memory_footprint / available_gpu * 100:.1f}% of {available_gpu}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca4473-9d65-4ea0-a7f5-9ee2155ac467",
   "metadata": {},
   "source": [
    "# SECTION 7: TOKENIZATION HANDS-ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce5b5217-622c-4291-9591-d0d6d06f91f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Section 7: Understanding Tokenization\n",
      "--------------------------------------------------------------------------------\n",
      "Question: Write a function to calculate the sum of a list\n",
      "Answer: def sum_list(lst):\n",
      "    return sum(lst)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "QUESTION TOKENIZATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Tokens: ['Write', 'Ġa', 'Ġfunction', 'Ġto', 'Ġcalculate', 'Ġthe', 'Ġsum', 'Ġof', 'Ġa', 'Ġlist']\n",
      "Token IDs: [7985, 264, 729, 311, 11047, 279, 2629, 315, 264, 1140]\n",
      "Number of tokens: 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANSWER TOKENIZATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Tokens: ['def', 'Ġsum', '_list', '(lst', '):Ċ', 'ĠĠĠ', 'Ġreturn', 'Ġsum', '(lst', ')']\n",
      "Token IDs: [750, 2629, 2019, 46046, 982, 262, 470, 2629, 46046, 8]\n",
      "Number of tokens: 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FULL SEQUENCE WITH SPECIAL TOKENS:\n",
      "--------------------------------------------------------------------------------\n",
      "Input IDs shape: torch.Size([1, 25])\n",
      "Input IDs: [14582, 25, 9645, 264, 729, 311, 11047, 279, 2629, 315, 264, 1140, 198, 16141, 25, 707, 2629, 2019, 46046, 982, 262, 470, 2629, 46046, 8]\n",
      "\n",
      "Decoded text: Question: Write a function to calculate the sum of a list\n",
      "Answer: def sum_list(lst):\n",
      "    return sum(lst)\n"
     ]
    }
   ],
   "source": [
    "print_title(7, \"Understanding Tokenization\")\n",
    "\n",
    "example_question = \"Write a function to calculate the sum of a list\"\n",
    "example_answer = \"def sum_list(lst):\\n    return sum(lst)\"\n",
    "\n",
    "print(f\"Question: {example_question}\")\n",
    "print(f\"Answer: {example_answer}\")\n",
    "\n",
    "q_tokens = tokenizer.tokenize(example_question)\n",
    "q_ids = tokenizer.encode(example_question, add_special_tokens=False)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"QUESTION TOKENIZATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Tokens: {q_tokens}\")\n",
    "print(f\"Token IDs: {q_ids}\")\n",
    "print(f\"Number of tokens: {len(q_ids)}\")\n",
    "\n",
    "a_tokens = tokenizer.tokenize(example_answer)\n",
    "a_ids = tokenizer.encode(example_answer, add_special_tokens=False)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ANSWER TOKENIZATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Tokens: {a_tokens}\")\n",
    "print(f\"Token IDs: {a_ids}\")\n",
    "print(f\"Number of tokens: {len(a_ids)}\")\n",
    "\n",
    "# bytre pair encoding: BPE builds its vocabulary by merging frequently co-occurring byte sequences.\n",
    "full_text = f\"Question: {example_question}\\nAnswer: {example_answer}\"\n",
    "full_encoding = tokenizer(full_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FULL SEQUENCE WITH SPECIAL TOKENS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Input IDs shape: {full_encoding['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {full_encoding['input_ids'][0].tolist()}\")\n",
    "\n",
    "decoded = tokenizer.decode(full_encoding['input_ids'][0])\n",
    "print(f\"\\nDecoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48568f9-c179-4a56-a197-faf6eaf03025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
